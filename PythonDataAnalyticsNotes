#import urllib.request
#url = 'HTTPS//:...'
#filename = 'Exemple.txt"
#urllib.request.retrieve(url, filename)

#install specific version of libraries used in  lab
#! mamba install pandas==1.3.3  -y
#! mamba install numpy=1.21.2 -y

#install specific version of libraries used in lab
#! mamba install pandas==1.3.3
#! mamba install numpy=1.21.2

# import pandas library
import pandas as pd
import numpy as np

#r+ : Reading and writing. Cannot truncate the file.
#w+ : Writing and reading. Truncates the file.
#a+ : Appending and Reading. Creates a new file, if none exists. You dont have to dwell on the specifics of each mode for this lab.




# create functions that will open files
# create functions that will read files
# create functions that will close files
# THE FOLLOWING DOES ALL 3
with open("example",r)asFile1:
	file_stuff = File1.read()
	print(file_stuff)
	print(File1.closed)
	print(file_stuff)


# Creating a method that will read lines
with open("example",r)asFile1:
	file_stuff = File1.readlines()
	print(file_stuff)


# creating a fuctional loop that will print out each line
with open("example",r)asFile1:
	for line in File1:
		print(line)


# create functions that will save files
def save_file():
    pass

# import data function
import pandas as pd
<PathSource> = "Path"

dataFrame = pd.read_csv(<PathSource>)

# if no header on data file 

dataFram = pd.read_csv(<PathSource>, header = None)

# saving new dataFrame 

path = '<PathDestination>'
df.to_csv(<path>)

# web scraper function to call other methods. 
def web_scraper(file):
    for address in file:
        if address != isinstance(re.file"\\, \:, https://+")
    open_file(file)
    read_file(file)
    save_file(file)
    close_file(file)


#Read all lines and save as list 
with open(example1, 'r') as file1:
	fileaslist = file1.readlines()


#writing to file
File1 = open(file name, 'w')as file1:
	file1.write("This is a line A")

#appending an existing file vs writing a new one
with open('filePATH', 'a') as File1:
	File1.write('<text>')
# To Open read and then write files
with open('filePATH', 'r') as readfile:
	with open('filePATH', 'w') as writefile:
		for line in readfiles:
			writefile.write(line)


# using counts method to identify how many of each category in the dataframe. 
<NewNameForCount> = df[<'columnName'>].value_counts()
NewNameForCount.rename(columns = {'oldColumnName':'newColumnName' inplace=True})
NewNameForCount.index.name = 'indexName'

#introducing a box plot for data visualization
sns.boxplot(x='nameOfHorizontalAxis',  y='nameOfVericalAxis', data=<nameOfDataFrame>)


#Scatter plotting using Matplotlib to identify relationships between two columns
#predictor X
#target Y

Y = dataframe['columnNamePredictor']
x = dataframe['columNameTarget']
plt.title("titleOfScatterPlot")
plt.xlable("pretictor")
plt.ylabel("target")

# Scatter plot with regression line seaborn reg plot to implement
sns.regplot(x='column1', y='column2', data = dataframe)
plt.ylim(0,)


#Group by in Py using Pandas
dataframe.Groupby()

newDataFrame = dataframe [['column1','column2','column3']]
dataFrameGroup = newDataFrame.groupby(['cloumn1','cloumn2'], as_index=False).mean()


#Pivot table displays one variable on the columns and one on the rows
df_PivotTable = dataFrameGroup.pivot(index='columnX','columnY')
# this will put columnZ as the individual instances.

#Heat map plot to target variable to relationships
plt.pcolor(dataframe_pivot, cmap="RdBu") #RED BLUE color scheme
plt.colorbar()
plt.show()


# ANOVA testing 
dataframe_ANOVA	= dataframe[['column1','column2']]
groupedANOVAdataframe = dataframe_ANOVA.groupb([['column1']])


#one line method for python SciPy package
anova_results = stats.f_oneway(grouped_anova.get_group('column1')['predictor'], grouped_anova.get_group('column2')['predictor'])
#F= score p=F_onewayResult(statistic=A), pvalue=B 
# A is large and B is over 1 so the results are significant. resulting in a categorical value vs a metric meaning there is a strong correlation.

/*
Pearson Correlation
-correlation coefficient 
	-close to +1: large positive relationship
	-close to -1: large negative relationship
	-close to 0: No relationship
-P-value
	- <0.001 Strong certainty
	- <0.05  Moderate certainty
	- <0.1   Weak certainty
	- >0.1   No certainty
*/

# import linear_model from scikit-learn
from sklearn.linear_model import LinearRegression
#create a linear regression object
lm=LinearRegression()

#define predictor variable and target variable 
x = df[['predictor']]
y = df[['target']]

# fit to model

lm.fit(x,y)

#Estimation for the prediction model called a Y hat
Yhat = lm.predict(x)

# intercept and slope are atributes of lm (b0) = lm.intercept_, (b1) = lm.coef_ #(aka:slope)

# Fitting a Multiple Linear Model Estimator

preditorVariables = df [['column1','column2','column3','column4'...]]
lm.fit(predictorVariables, df['target']
Yhat = lm.predict(X)


#Residual Plot
import seaborn as sns
sns.residplot)df['predictor'], df['target'])

#Distribution Plot
import seabord as sns
ax1 = sns.distplot(df['target'], hist=False, color='', label='<label>')
sns.distplot(Yhat, hist=False, clor='b', lable='<Yhat>', ax=ax1)

Polynomial Regression
f.np.polyfit(x,y,3)
p=np.polyd(f)
print(p)#

#Polynomial regression with more than one dimension
#in scikit-learn
from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=2, include_bias=False)
x_polly=pr.fit_transform(x[['predictor','predictor2']])


#Pre-processing normalizing the data for multiple predictors at once
from sklearn.preprocessing import StandardScaler
SCALE =StandardScaler()
SCALE..fit(x_data[['predictor1','predictor2']])
x_scale=SCALE.transform(x_data[['predictor1','predictor2']])


#Piplines
from sklean.preprocessing import PolynomialFeatures
from sklean.linear_model import LinearRegression
from sklean.preprocessing import StandardScaler

from sklearn.pipeline import Pipeline
#create a list of tuples
Input=[('Scale',StandardScaler()),('Polynomials', PolynomialFeatures(degree=2)),('Model',LinearRegression())]

#Train the Pipeline
Pipe.train(x['predictor1','predictor2','predictor3','predictor4'],y)
Yhat=Pipe.predict(x[['predictor1','predictor2','predictor3','predictor4']])

/*
Two methods to measure for in-sample evaluation
Measures for insample evaluation
-MSE(MeanSquaredError)
-R2(R-squared) How close the data is to the regression line
*/

#MSE Calculation
from sklean.metrics import mean_squared_error
mean_squared_error(df['target'],Y_predict_simple_fit)

# Finding R2 value usually between 0 and 1
X= df[['predictor']]
Y=[['target']]

lm.score(X,Y)


#generating a sequence 
new_input=np.arange(<start>,<stop>,<step>),reshape(-1,1)
